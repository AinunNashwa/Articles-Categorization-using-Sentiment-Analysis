# -*- coding: utf-8 -*-
"""cat_articles_sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17f1MmYmV1VasDQz-L_TsSz4aD_bXEn9J
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Jun 15 10:16:48 2022

This script is about to categorize the articles into 5 different subjects, namely
Sport, Tech, Business, Entertainment and Politics using Sentiment Analysis

['tech', 'business', 'sport', 'entertainment', 'politics']

@author: Ainun Nashwa
"""

!cp /content/drive/MyDrive/Colab\ Notebooks/modules_for_cat_articles.py /content
from modules_for_cat_articles import ModelCreation,model_evaluation

from sklearn.metrics import confusion_matrix,classification_report
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Bidirectional,Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import LSTM,Dense,Dropout
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import TensorBoard
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import plot_model
from sklearn.metrics import accuracy_score
from tensorflow.keras import Input

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime 
import pickle
import json
import os
import re

#%% Statics
CSV_URL = 'https://raw.githubusercontent.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/master/bbc-text.csv'

log_dir = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
LOG_FOLDER_PATH = os.path.join(os.getcwd(), '/content/sample_data/Articles_Categorization/log_sentiment_analysis',log_dir)
OHE_PICKLE_PATH = os.path.join(os.getcwd(), 'ohe_category.pkl')
MODEL_SAVE_PATH = os.path.join(os.getcwd(),'model_cat_sentiment.h5')
TOKENIZER_PATH = os.path.join(os.getcwd(),'tokenizer_cat_sentiment.json')

#%% EDA
# Step 1: Data Loading

df = pd.read_csv(CSV_URL)

df_copy = df.copy()

# Step 2: Data inspection
df.info()
print(df.head(10))
print(df.tail(10))

# character: number, $, £, $11bn £5.8bn, (), s = ' '

df.duplicated().sum() # 99
df[df.duplicated()]

df['category'].unique()

text = df['text'].values # features: X
category = df['category'].values # category:y

# Step 3: Data Cleaning
# to remove duplicated data
df = df.drop_duplicates()

# To remove unnecessary characters  
def remove_special_characters(text):
  pat = r'[^a-zA-z]'  
  text = re.sub(pat, ' ', text)     
  return text

# to check
remove_special_characters(df['text'][1])

# To remove unnecessary characters  
def remove_extra_whitespace_tabs(text):
  pattern = r'^\s*|\s\s*'
  text = re.sub(pattern, ' ', text).strip()
  return text

# to check
remove_extra_whitespace_tabs(df['text'][1])

# Step 4: Features Selection --> nothing to select
df_zero = df['text'][0].split()
len(df_zero)

# Step 5: Preprocessing
# Convert into Tokenizer

vocab_size = 10000
oov_token = 'OOV'
tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_token)

# Steps to make sure model learn all of the words
tokenizer.fit_on_texts(text) 
word_index = tokenizer.word_index

# Step to convert into numbers
train_sequences = tokenizer.texts_to_sequences(text)

# Steps for Padding & truncating

length_of_text = [len(i) for i in train_sequences]
print('Max Length: ',np.max(length_of_text))

print('Min Length: ',np.min(length_of_text))

print('Median Length: ',np.median(length_of_text))

print('Mean Length: ',np.mean(length_of_text))

max_len = int(np.median(length_of_text))


padded_text = pad_sequences(train_sequences,
                              maxlen=max_len,
                              padding='post',
                              truncating='post')

# Step One Hot Encoding for the target

ohe = OneHotEncoder(sparse=False)
category = ohe.fit_transform(np.expand_dims(category,axis=-1))

with open (OHE_PICKLE_PATH, 'wb') as file:
    pickle.dump(ohe,file)

# Step for Train test split

X_train,X_test,y_train,y_test = train_test_split(padded_text,
                                                 category,
                                                 test_size=0.3,
                                                 random_state=3)

X_train = np.expand_dims(X_train,axis=-1)
X_test = np.expand_dims(X_test,axis=-1)

# callback

tensorboard_callback = TensorBoard(log_dir=LOG_FOLDER_PATH)

#%% Model Development
# use input, LSTM layers,dropout,dense
# achieve > 70% accuracy and 0.7 f1 score

mc = ModelCreation()
model = mc.sequential_layer(num_node=128,drop_rate=0.2,output_node=5)

plot_model(model,show_shapes=True,show_layer_names=(True))

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics='acc')

hist = model.fit(X_train,y_train,
                 batch_size=128,
                 validation_data=(X_test,y_test),
                 epochs=10,
                 callbacks=tensorboard_callback)

#%% Model Evaluation

print(hist.history.keys())

me = model_evaluation()
me.plot_graph(hist)

#%% Model Analysis

results = model.evaluate(X_test,y_test)
print(results)

y_true = y_test
y_pred = model.predict(X_test)

y_true = np.argmax(y_test,axis=1)
y_pred = np.argmax(y_pred,axis=1)

#%% Accuracy
cr = classification_report(y_true,y_pred)
acc_score = accuracy_score(y_true,y_pred)
cm = confusion_matrix(y_true,y_pred)

print(cr)
print('Accuracy Score: ', acc_score)
print(cm)

labels = ['tech', 'business', 'sport', 'entertainment', 'politics']
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=labels)
disp.plot(cmap=plt.cm.Reds)
plt.show()

#%% Model Saving

model.save(MODEL_SAVE_PATH)

token_json = tokenizer.to_json()
with open(TOKENIZER_PATH, 'w') as file:
    json.dump(token_json,file)

#%% Discussion
# The datasets contain duplicates, unnecessary characters/spaces and numbers
# Using RegEx to remove unnecessary characters/spaces and numbers
# Model being trained using Embedding,Bidirectional,LSTM and Dense Layer
# Model achieved approximately 93% accuracy
# Recall and f1 score reports around 89% to 90% accuracy
# However the model starts to overfit after 4th epoch
# Early stopping can be introduced in future to prevent overfitting
# Increasing the drop out rate also can control overfitting
# Increase the model complexity means more layer introduced to model that can increase the accuracy
# Further improvement during preprocessing data is by applying : Stemming, Lemmatization, Stopwords
# For model impovement: try with different architecture for example BERT model, transformer
# model, GPT3 may help to improve the model

# Commented out IPython magic to ensure Python compatibility.
#%% Tensorboard
# %load_ext tensorboard
# %reload_ext tensorboard
# %tensorboard --logdir /content/sample_data/Articles_Categorization/log_sentiment_analysis